{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farfetech case study\n",
    "\n",
    "Product description generation.\n",
    "\n",
    "The object of this script is using deep learning technologies (CNN, LSTM) for product description generation.\n",
    "\n",
    "- Author: Kai Chen\n",
    "- Date: Apr, 2018\n",
    "\n",
    "\n",
    "### Reference\n",
    "- https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import operator\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import callbacks, applications, optimizers\n",
    "from keras.models import load_model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Define the file paths\n",
    "\n",
    "PRODUCT_CSV_FILE = 'data/products.csv'\n",
    "ATTRIBUTE_CSV_FILE = 'data/attributes.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: read and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Description</th>\n",
       "      <th>DescriptionDate</th>\n",
       "      <th>SeasonOriginal</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Family</th>\n",
       "      <th>Category</th>\n",
       "      <th>ArticlePhotoId</th>\n",
       "      <th>CreateDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11295277</td>\n",
       "      <td>VIPE6CE-169953MCC 38NO</td>\n",
       "      <td>2016-01-07 13:13:09.527</td>\n",
       "      <td>SS15</td>\n",
       "      <td>Celine Black Phantom Bag</td>\n",
       "      <td>CELINE COLLARD</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Tote Bags</td>\n",
       "      <td>6129459</td>\n",
       "      <td>2016-01-07 13:10:46.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11292059</td>\n",
       "      <td>Grey  cotton 'Skip' crewneck from S.N.S. Herni...</td>\n",
       "      <td>2016-01-08 14:45:59.673</td>\n",
       "      <td>SS16</td>\n",
       "      <td>'Skip' crewneck</td>\n",
       "      <td>S.N.S. HERNING</td>\n",
       "      <td>MEN</td>\n",
       "      <td>GREY</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Sweaters &amp; Knitwear</td>\n",
       "      <td>6156126</td>\n",
       "      <td>2016-01-04 19:52:05.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11290981</td>\n",
       "      <td>Multicolour cotton 'Pak’r Tatenda' backpack fr...</td>\n",
       "      <td>2016-01-11 19:27:45.330</td>\n",
       "      <td>SS16</td>\n",
       "      <td>'Pakr Tatenda' backpack</td>\n",
       "      <td>EASTPAK</td>\n",
       "      <td>UNISEX</td>\n",
       "      <td>YELLOW &amp; ORANGE</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>6216609</td>\n",
       "      <td>2016-01-03 15:21:20.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11293179</td>\n",
       "      <td>Ivory white and black hemp-cotton blend 'Honey...</td>\n",
       "      <td>2016-01-13 11:33:11.150</td>\n",
       "      <td>SS16</td>\n",
       "      <td>'Honey' wide brim hat</td>\n",
       "      <td>EUGENIA KIM</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Hats</td>\n",
       "      <td>6199465</td>\n",
       "      <td>2016-01-05 18:08:57.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11293099</td>\n",
       "      <td>Ivory white cotton embroidered lace frilled dr...</td>\n",
       "      <td>2016-01-13 15:22:08.247</td>\n",
       "      <td>SS16</td>\n",
       "      <td>embroidered lace frilled dress</td>\n",
       "      <td>RED VALENTINO</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>6173011</td>\n",
       "      <td>2016-01-05 16:55:35.427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductId                                        Description  \\\n",
       "0   11295277                             VIPE6CE-169953MCC 38NO   \n",
       "1   11292059  Grey  cotton 'Skip' crewneck from S.N.S. Herni...   \n",
       "2   11290981  Multicolour cotton 'Pak’r Tatenda' backpack fr...   \n",
       "3   11293179  Ivory white and black hemp-cotton blend 'Honey...   \n",
       "4   11293099  Ivory white cotton embroidered lace frilled dr...   \n",
       "\n",
       "           DescriptionDate SeasonOriginal                     ProductName  \\\n",
       "0  2016-01-07 13:13:09.527           SS15        Celine Black Phantom Bag   \n",
       "1  2016-01-08 14:45:59.673           SS16                 'Skip' crewneck   \n",
       "2  2016-01-11 19:27:45.330           SS16        'Pakr Tatenda' backpack   \n",
       "3  2016-01-13 11:33:11.150           SS16           'Honey' wide brim hat   \n",
       "4  2016-01-13 15:22:08.247           SS16  embroidered lace frilled dress   \n",
       "\n",
       "            Brand  Gender           Colour       Family             Category  \\\n",
       "0  CELINE COLLARD   WOMEN            BLACK         Bags            Tote Bags   \n",
       "1  S.N.S. HERNING     MEN             GREY     Clothing  Sweaters & Knitwear   \n",
       "2         EASTPAK  UNISEX  YELLOW & ORANGE         Bags            Backpacks   \n",
       "3     EUGENIA KIM   WOMEN            WHITE  Accessories                 Hats   \n",
       "4   RED VALENTINO   WOMEN            WHITE     Clothing              Dresses   \n",
       "\n",
       "   ArticlePhotoId               CreateDate  \n",
       "0         6129459  2016-01-07 13:10:46.507  \n",
       "1         6156126  2016-01-04 19:52:05.203  \n",
       "2         6216609  2016-01-03 15:21:20.480  \n",
       "3         6199465  2016-01-05 18:08:57.317  \n",
       "4         6173011  2016-01-05 16:55:35.427  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12631, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_product = pd.read_csv(PRODUCT_CSV_FILE)\n",
    "\n",
    "display(df_product.head(5))\n",
    "display(df_product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products 12631 in the csv file\n"
     ]
    }
   ],
   "source": [
    "list_product_id_df = df_product['ProductId'].unique()\n",
    "list_product_id_df = np.array(list_product_id_df)\n",
    "\n",
    "print('number of products {} in the csv file'.format(list_product_id_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with key: photo id -> value: product id\n",
    "# Note one photo belongs only to one product\n",
    "\n",
    "list_photo_id = df_product['ArticlePhotoId'].unique()\n",
    "\n",
    "dict_photo_product_id = dict()\n",
    "\n",
    "for photo_id in list_photo_id:\n",
    "    dict_photo_product_id[photo_id] = df_product[df_product['ArticlePhotoId']==photo_id]['ProductId'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products: 12436\n"
     ]
    }
   ],
   "source": [
    "# Update the list_product_id, such that each product should have an image\n",
    "\n",
    "list_product_id = []\n",
    "\n",
    "# img_width, img_height = 100, 100\n",
    "# img_dir_path = \"data/images_{}_{}/\".format(img_width, img_height)\n",
    "# img_width, img_height = 100, 100\n",
    "img_dir_path = \"data/images/\"\n",
    "\n",
    "dirs = os.listdir(img_dir_path)\n",
    "\n",
    "for file_name in dirs:\n",
    "    file_path = os.path.join(img_dir_path, file_name)\n",
    "    product_id = int(file_name.split('_')[0])\n",
    "\n",
    "    if not product_id in list_product_id_df:\n",
    "        print('photo {} does not have product information'.format(file_path))\n",
    "    else:\n",
    "        list_product_id.append(product_id)\n",
    "    \n",
    "# print(list_product_id)\n",
    "print('number of products: {}'.format(len(list_product_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: image data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing the image data ...')\n",
    "\n",
    "# extract VGG16 features\n",
    "def extract_features(dict_product_img):\n",
    "    # model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, nb_channel))\n",
    "    model = applications.VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print(model.summary())\n",
    "    features = dict()\n",
    "    for product_id, img in dict_product_img.items():\n",
    "        feature = model.predict(img, verbose=0)\n",
    "        features[product_id] = feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# Create a dictionary with\n",
    "# key: product id, value: image\n",
    "dict_product_img = dict()\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "img_dir_path = \"data/images/\"\n",
    "dirs = os.listdir(img_dir_path)\n",
    "\n",
    "product_image_feature_file_path = 'product-vgg-features.pkl'\n",
    "\n",
    "for file_name in dirs:\n",
    "    file_path = os.path.join(img_dir_path, file_name)\n",
    "\n",
    "    # img = load_img(file_path)         \n",
    "    img = load_img(file_path, target_size=(img_width, img_height))   # this is a PIL image\n",
    "    x = img_to_array(img)                                            # this is a Numpy array with shape (img_width, img_height, 3)\n",
    "    x = x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))           # this is a Numpy array with shape (1, 3, img_width, img_height)\n",
    "    # x = x.reshape((1,) + x.shape)                                  \n",
    "    # prepare the image for the VGG model\n",
    "    x = preprocess_input(x)\n",
    "    product_id = int(file_name.split('_')[0])\n",
    "\n",
    "    if not int(product_id) in list_product_id:\n",
    "        print('photo {} does not have product information'.format(file_path))\n",
    "    else:\n",
    "        dict_product_img[product_id] = x\n",
    "\n",
    "for product_id in list_product_id_df:\n",
    "    if product_id not in dict_product_img:\n",
    "        print('product {} does not have an image'.format(product_id))\n",
    "\n",
    "# extract VGG16 features\n",
    "dict_product_img_features = extract_features(dict_product_img)\n",
    "# save the features to file\n",
    "dump(dict_product_img_features, open(product_image_feature_file_path, 'wb'))\n",
    "\n",
    "print('save product image features to {}'.format(product_image_feature_file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing text data ...')\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    \"\"\"\n",
    "    https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "    - Convert all words to lowercase.\n",
    "    - Remove all punctuation.\n",
    "    - Remove all words that are one character or less in length (e.g. ‘a’).\n",
    "    - Remove all words with numbers in them.\n",
    "    \"\"\"\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word) > 1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        clean_str = ' '.join(desc)\n",
    "        if not clean_str:\n",
    "            print('cleaned description of product {} is empty'.format(key))\n",
    "        else:\n",
    "            descriptions[key] = clean_str\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    \"\"\"\n",
    "    convert the loaded descriptions into a vocabulary of words\n",
    "    \"\"\"\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "\n",
    "def save_descriptions(descriptions, filename):\n",
    "    \"\"\"\n",
    "    save descriptions to file, one per line\n",
    "    \"\"\"\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        if not desc:\n",
    "            print('product {} does not have a description'.format(key))\n",
    "        # print(key)\n",
    "        # print(desc)\n",
    "        lines.append(str(key) + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "product_description_file_path = 'product-descriptions.txt'\n",
    "\n",
    "# create a dictionary with key: product id -> value: description\n",
    "dict_product_des = dict()\n",
    "\n",
    "for product_id in list_product_id:\n",
    "    # we assume that one product has only one description.\n",
    "    if product_id in dict_product_des:\n",
    "        print('product {} has more than one description'.format(product_id))\n",
    "    description = df_product[df_product['ProductId']==product_id]['Description'].values[0]\n",
    "    if not description:\n",
    "        print('product {} does not have a description'.format(product_id))\n",
    "    else:\n",
    "        dict_product_des[product_id] = description\n",
    "\n",
    "print('before clean')\n",
    "for product_id in list_product_id[0:5]:\n",
    "    print(dict_product_des[product_id])\n",
    "\n",
    "# clean the descriptions\n",
    "clean_descriptions(dict_product_des)\n",
    "\n",
    "print('after clean')\n",
    "for product_id in list_product_id[0:5]:\n",
    "    print(dict_product_des[product_id])\n",
    "\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(dict_product_des)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "# save descriptions\n",
    "save_descriptions(dict_product_des, product_description_file_path)\n",
    "\n",
    "print('save product description to {}'.format(product_description_file_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: prepare the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    load doc into memory\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_clean_descriptions(filename, list_product_id):\n",
    "    \"\"\"\n",
    "    load clean descriptions into memory\n",
    "    \"\"\"\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        product_id, product_desc = int(tokens[0]), tokens[1:]\n",
    "        if product_id in list_product_id:\n",
    "            if product_id not in descriptions:\n",
    "                descriptions[product_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(product_desc) + ' endseq'\n",
    "            descriptions[product_id].append(desc)\n",
    "\n",
    "    # for key, value in descriptions.items():\n",
    "    #     print(key)\n",
    "    #     print(value)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "\n",
    "def load_photo_features(filename, list_product_id):\n",
    "    \"\"\"\n",
    "    load photo features\n",
    "    \"\"\"\n",
    "    \n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "\n",
    "    # features = {k: all_features[k] for k in list_product_id}\n",
    "\n",
    "    # filter features\n",
    "    dataset = []\n",
    "    # dict_features = dict()\n",
    "    for product_id in list_product_id:\n",
    "        if (str(product_id) in all_features) or (product_id in all_features):\n",
    "            dataset.append(product_id)\n",
    "\n",
    "    # for product_id, features in all_features.items():\n",
    "    #     if int(product_id) in list_product_id:\n",
    "    #         dict_features[int(product_id)] = features\n",
    "\n",
    "    # filter features\n",
    "    features = {int(k): all_features[k] for k in dataset}\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def to_lines(descriptions):\n",
    "    \"\"\"\n",
    "    convert a dictionary of clean descriptions to a list of descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    fit a tokenizer given caption descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def max_length(descriptions):\n",
    "    \"\"\"\n",
    "    calculate the length of the description with the most words\n",
    "    \"\"\"\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "\n",
    "# ---------\n",
    "# encode the text\n",
    "# for example, the input sequence “little girl running in field” would be split into 6 input-output pairs to train the model:\n",
    "\"\"\"\n",
    "X1,\t\tX2 (text sequence), \t\t\t\t\t\t y (word)\n",
    "photo\tstartseq, \t\t\t\t\t\t\t\t\t little\n",
    "photo\tstartseq, little,\t\t\t\t\t\t\t girl\n",
    "photo\tstartseq, little, girl, \t\t\t\t\t running\n",
    "photo\tstartseq, little, girl, running, \t\t\t in\n",
    "photo\tstartseq, little, girl, running, in, \t\t field\n",
    "photo\tstartseq, little, girl, running, in, field,  endseq\n",
    "\"\"\"\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    \"\"\"\n",
    "    create sequences of images, input sequences and output words for an image\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    data generator, intended to be used in a call to model.fit_generator()\n",
    "    \"\"\"\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            if key in photos:\n",
    "                photo = photos[key][0]\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "                yield [[in_img, in_seq], out_word]\n",
    "                \n",
    "\n",
    "# image features\n",
    "all_features = load_photo_features(product_image_feature_file_path, list_product_id)\n",
    "print('All image features: %d' % len(all_features))\n",
    "\n",
    "# descriptions\n",
    "all_descriptions = load_clean_descriptions(product_description_file_path, list_product_id)\n",
    "print('Descriptions: %d' % len(all_descriptions))\n",
    "\n",
    "# print('Descriptions')\n",
    "# for key, value in all_descriptions.items():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "# prepare train and test sets\n",
    "percentage_train = 0.9\n",
    "list_train_product_id = list_product_id[0:int(len(list_product_id)*percentage_train)]\n",
    "list_test_product_id = list_product_id[len(list_train_product_id):]\n",
    "\n",
    "train_features = dict()\n",
    "train_descriptions = dict()\n",
    "for product_id in list_train_product_id:\n",
    "    train_features[product_id] = all_features[product_id]\n",
    "    train_descriptions[product_id] = all_descriptions[product_id]\n",
    "\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "test_features = dict()\n",
    "test_descriptions = dict()\n",
    "for product_id in list_test_product_id:\n",
    "    test_features[product_id] = all_features[product_id]\n",
    "    test_descriptions[product_id] = all_descriptions[product_id]\n",
    "\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "\n",
    "# prepare sequences\n",
    "# X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(all_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(all_descriptions)\n",
    "print('Description Length: %d' % max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    define the captioning model\n",
    "    \"\"\"\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train model ... ')\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 2\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    model_path = 'model_' + str(i) + '.h5'\n",
    "    model.save(model_path)\n",
    "    print('save model to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
