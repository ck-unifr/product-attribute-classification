{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farfetech case study\n",
    "\n",
    "Product description generation.\n",
    "\n",
    "The object of this script is using deep learning technologies (CNN, LSTM) for product description generation.\n",
    "\n",
    "- Author: Kai Chen\n",
    "- Date: Apr, 2018\n",
    "\n",
    "\n",
    "### Reference\n",
    "- https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import operator\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import callbacks, applications, optimizers\n",
    "from keras.models import load_model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Define the file paths\n",
    "\n",
    "PRODUCT_CSV_FILE = 'data/products.csv'\n",
    "ATTRIBUTE_CSV_FILE = 'data/attributes.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: read and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Description</th>\n",
       "      <th>DescriptionDate</th>\n",
       "      <th>SeasonOriginal</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Family</th>\n",
       "      <th>Category</th>\n",
       "      <th>ArticlePhotoId</th>\n",
       "      <th>CreateDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11295277</td>\n",
       "      <td>VIPE6CE-169953MCC 38NO</td>\n",
       "      <td>2016-01-07 13:13:09.527</td>\n",
       "      <td>SS15</td>\n",
       "      <td>Celine Black Phantom Bag</td>\n",
       "      <td>CELINE COLLARD</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Tote Bags</td>\n",
       "      <td>6129459</td>\n",
       "      <td>2016-01-07 13:10:46.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11292059</td>\n",
       "      <td>Grey  cotton 'Skip' crewneck from S.N.S. Herni...</td>\n",
       "      <td>2016-01-08 14:45:59.673</td>\n",
       "      <td>SS16</td>\n",
       "      <td>'Skip' crewneck</td>\n",
       "      <td>S.N.S. HERNING</td>\n",
       "      <td>MEN</td>\n",
       "      <td>GREY</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Sweaters &amp; Knitwear</td>\n",
       "      <td>6156126</td>\n",
       "      <td>2016-01-04 19:52:05.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11290981</td>\n",
       "      <td>Multicolour cotton 'Pak’r Tatenda' backpack fr...</td>\n",
       "      <td>2016-01-11 19:27:45.330</td>\n",
       "      <td>SS16</td>\n",
       "      <td>'Pakr Tatenda' backpack</td>\n",
       "      <td>EASTPAK</td>\n",
       "      <td>UNISEX</td>\n",
       "      <td>YELLOW &amp; ORANGE</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Backpacks</td>\n",
       "      <td>6216609</td>\n",
       "      <td>2016-01-03 15:21:20.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11293179</td>\n",
       "      <td>Ivory white and black hemp-cotton blend 'Honey...</td>\n",
       "      <td>2016-01-13 11:33:11.150</td>\n",
       "      <td>SS16</td>\n",
       "      <td>'Honey' wide brim hat</td>\n",
       "      <td>EUGENIA KIM</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Hats</td>\n",
       "      <td>6199465</td>\n",
       "      <td>2016-01-05 18:08:57.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11293099</td>\n",
       "      <td>Ivory white cotton embroidered lace frilled dr...</td>\n",
       "      <td>2016-01-13 15:22:08.247</td>\n",
       "      <td>SS16</td>\n",
       "      <td>embroidered lace frilled dress</td>\n",
       "      <td>RED VALENTINO</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>6173011</td>\n",
       "      <td>2016-01-05 16:55:35.427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductId                                        Description  \\\n",
       "0   11295277                             VIPE6CE-169953MCC 38NO   \n",
       "1   11292059  Grey  cotton 'Skip' crewneck from S.N.S. Herni...   \n",
       "2   11290981  Multicolour cotton 'Pak’r Tatenda' backpack fr...   \n",
       "3   11293179  Ivory white and black hemp-cotton blend 'Honey...   \n",
       "4   11293099  Ivory white cotton embroidered lace frilled dr...   \n",
       "\n",
       "           DescriptionDate SeasonOriginal                     ProductName  \\\n",
       "0  2016-01-07 13:13:09.527           SS15        Celine Black Phantom Bag   \n",
       "1  2016-01-08 14:45:59.673           SS16                 'Skip' crewneck   \n",
       "2  2016-01-11 19:27:45.330           SS16        'Pakr Tatenda' backpack   \n",
       "3  2016-01-13 11:33:11.150           SS16           'Honey' wide brim hat   \n",
       "4  2016-01-13 15:22:08.247           SS16  embroidered lace frilled dress   \n",
       "\n",
       "            Brand  Gender           Colour       Family             Category  \\\n",
       "0  CELINE COLLARD   WOMEN            BLACK         Bags            Tote Bags   \n",
       "1  S.N.S. HERNING     MEN             GREY     Clothing  Sweaters & Knitwear   \n",
       "2         EASTPAK  UNISEX  YELLOW & ORANGE         Bags            Backpacks   \n",
       "3     EUGENIA KIM   WOMEN            WHITE  Accessories                 Hats   \n",
       "4   RED VALENTINO   WOMEN            WHITE     Clothing              Dresses   \n",
       "\n",
       "   ArticlePhotoId               CreateDate  \n",
       "0         6129459  2016-01-07 13:10:46.507  \n",
       "1         6156126  2016-01-04 19:52:05.203  \n",
       "2         6216609  2016-01-03 15:21:20.480  \n",
       "3         6199465  2016-01-05 18:08:57.317  \n",
       "4         6173011  2016-01-05 16:55:35.427  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12631, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_product = pd.read_csv(PRODUCT_CSV_FILE)\n",
    "\n",
    "display(df_product.head(5))\n",
    "display(df_product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products 12631 in the csv file\n"
     ]
    }
   ],
   "source": [
    "list_product_id_df = df_product['ProductId'].unique()\n",
    "list_product_id_df = np.array(list_product_id_df)\n",
    "\n",
    "print('number of products {} in the csv file'.format(list_product_id_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with key: photo id -> value: product id\n",
    "# Note one photo belongs only to one product\n",
    "\n",
    "list_photo_id = df_product['ArticlePhotoId'].unique()\n",
    "\n",
    "dict_photo_product_id = dict()\n",
    "\n",
    "for photo_id in list_photo_id:\n",
    "    dict_photo_product_id[photo_id] = df_product[df_product['ArticlePhotoId']==photo_id]['ProductId'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products: 12436\n"
     ]
    }
   ],
   "source": [
    "# Update the list_product_id, such that each product should have an image\n",
    "\n",
    "list_product_id = []\n",
    "\n",
    "# img_width, img_height = 100, 100\n",
    "# img_dir_path = \"data/images_{}_{}/\".format(img_width, img_height)\n",
    "# img_width, img_height = 100, 100\n",
    "img_dir_path = \"data/images/\"\n",
    "\n",
    "dirs = os.listdir(img_dir_path)\n",
    "\n",
    "for file_name in dirs:\n",
    "    file_path = os.path.join(img_dir_path, file_name)\n",
    "    product_id = int(file_name.split('_')[0])\n",
    "\n",
    "    if not product_id in list_product_id_df:\n",
    "        print('photo {} does not have product information'.format(file_path))\n",
    "    else:\n",
    "        list_product_id.append(product_id)\n",
    "    \n",
    "# print(list_product_id)\n",
    "print('number of products: {}'.format(len(list_product_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: image data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the image data ...\n",
      "product 11075939 does not have an image\n",
      "product 11256262 does not have an image\n",
      "product 11289185 does not have an image\n",
      "product 11281815 does not have an image\n",
      "product 11262710 does not have an image\n",
      "product 11299211 does not have an image\n",
      "product 11281607 does not have an image\n",
      "product 11312010 does not have an image\n",
      "product 11292693 does not have an image\n",
      "product 11279245 does not have an image\n",
      "product 11316300 does not have an image\n",
      "product 11212774 does not have an image\n",
      "product 11212851 does not have an image\n",
      "product 11321494 does not have an image\n",
      "product 11305007 does not have an image\n",
      "product 11318537 does not have an image\n",
      "product 11295266 does not have an image\n",
      "product 11326989 does not have an image\n",
      "product 11319413 does not have an image\n",
      "product 10949330 does not have an image\n",
      "product 11273587 does not have an image\n",
      "product 11329645 does not have an image\n",
      "product 11341121 does not have an image\n",
      "product 11343689 does not have an image\n",
      "product 11340126 does not have an image\n",
      "product 11412983 does not have an image\n",
      "product 11260582 does not have an image\n",
      "product 11357136 does not have an image\n",
      "product 11254200 does not have an image\n",
      "product 11306333 does not have an image\n",
      "product 11376688 does not have an image\n",
      "product 11369769 does not have an image\n",
      "product 11355288 does not have an image\n",
      "product 11295038 does not have an image\n",
      "product 11413285 does not have an image\n",
      "product 11350199 does not have an image\n",
      "product 11354415 does not have an image\n",
      "product 11352350 does not have an image\n",
      "product 11414445 does not have an image\n",
      "product 11341898 does not have an image\n",
      "product 11435920 does not have an image\n",
      "product 11440274 does not have an image\n",
      "product 11343178 does not have an image\n",
      "product 11408457 does not have an image\n",
      "product 11280121 does not have an image\n",
      "product 11264950 does not have an image\n",
      "product 11360756 does not have an image\n",
      "product 11060962 does not have an image\n",
      "product 11202177 does not have an image\n",
      "product 11401559 does not have an image\n",
      "product 11179835 does not have an image\n",
      "product 11495632 does not have an image\n",
      "product 11327066 does not have an image\n",
      "product 11437124 does not have an image\n",
      "product 11496221 does not have an image\n",
      "product 11320336 does not have an image\n",
      "product 11298733 does not have an image\n",
      "product 11219903 does not have an image\n",
      "product 10729390 does not have an image\n",
      "product 11410207 does not have an image\n",
      "product 11505636 does not have an image\n",
      "product 11502912 does not have an image\n",
      "product 11456532 does not have an image\n",
      "product 11393072 does not have an image\n",
      "product 11528947 does not have an image\n",
      "product 11493573 does not have an image\n",
      "product 11409873 does not have an image\n",
      "product 11514920 does not have an image\n",
      "product 10587565 does not have an image\n",
      "product 11464912 does not have an image\n",
      "product 11447231 does not have an image\n",
      "product 11512638 does not have an image\n",
      "product 11310426 does not have an image\n",
      "product 11515676 does not have an image\n",
      "product 11495077 does not have an image\n",
      "product 11548428 does not have an image\n",
      "product 11346729 does not have an image\n",
      "product 11514958 does not have an image\n",
      "product 10796811 does not have an image\n",
      "product 11364815 does not have an image\n",
      "product 11358925 does not have an image\n",
      "product 11585483 does not have an image\n",
      "product 11486371 does not have an image\n",
      "product 11573249 does not have an image\n",
      "product 11368694 does not have an image\n",
      "product 11599442 does not have an image\n",
      "product 11546877 does not have an image\n",
      "product 11596564 does not have an image\n",
      "product 11539668 does not have an image\n",
      "product 11252062 does not have an image\n",
      "product 11514491 does not have an image\n",
      "product 11525963 does not have an image\n",
      "product 11582565 does not have an image\n",
      "product 11542190 does not have an image\n",
      "product 11516214 does not have an image\n",
      "product 11522747 does not have an image\n",
      "product 11549180 does not have an image\n",
      "product 11500109 does not have an image\n",
      "product 11560515 does not have an image\n",
      "product 11587608 does not have an image\n",
      "product 11639862 does not have an image\n",
      "product 11606679 does not have an image\n",
      "product 11612880 does not have an image\n",
      "product 11583379 does not have an image\n",
      "product 11445860 does not have an image\n",
      "product 11604371 does not have an image\n",
      "product 11517085 does not have an image\n",
      "product 11528768 does not have an image\n",
      "product 11611983 does not have an image\n",
      "product 11266575 does not have an image\n",
      "product 11617112 does not have an image\n",
      "product 11157897 does not have an image\n",
      "product 11630211 does not have an image\n",
      "product 11628912 does not have an image\n",
      "product 11513797 does not have an image\n",
      "product 11651734 does not have an image\n",
      "product 11649644 does not have an image\n",
      "product 11627420 does not have an image\n",
      "product 10872545 does not have an image\n",
      "product 11626725 does not have an image\n",
      "product 11651172 does not have an image\n",
      "product 11668944 does not have an image\n",
      "product 11360535 does not have an image\n",
      "product 11592435 does not have an image\n",
      "product 11566687 does not have an image\n",
      "product 11506289 does not have an image\n",
      "product 11446920 does not have an image\n",
      "product 11637782 does not have an image\n",
      "product 11685339 does not have an image\n",
      "product 11560004 does not have an image\n",
      "product 11535118 does not have an image\n",
      "product 11135173 does not have an image\n",
      "product 11504893 does not have an image\n",
      "product 11502508 does not have an image\n",
      "product 11298944 does not have an image\n",
      "product 11765683 does not have an image\n",
      "product 11769905 does not have an image\n",
      "product 11777770 does not have an image\n",
      "product 11791606 does not have an image\n",
      "product 11697681 does not have an image\n",
      "product 11695788 does not have an image\n",
      "product 11756375 does not have an image\n",
      "product 11773349 does not have an image\n",
      "product 11736921 does not have an image\n",
      "product 11820060 does not have an image\n",
      "product 11545915 does not have an image\n",
      "product 11780785 does not have an image\n",
      "product 11509461 does not have an image\n",
      "product 11773322 does not have an image\n",
      "product 11792853 does not have an image\n",
      "product 11664670 does not have an image\n",
      "product 11855960 does not have an image\n",
      "product 11768861 does not have an image\n",
      "product 11729453 does not have an image\n",
      "product 11541337 does not have an image\n",
      "product 11653883 does not have an image\n",
      "product 11785432 does not have an image\n",
      "product 11847496 does not have an image\n",
      "product 11511871 does not have an image\n",
      "product 11612675 does not have an image\n",
      "product 11672069 does not have an image\n",
      "product 11841811 does not have an image\n",
      "product 11783469 does not have an image\n",
      "product 11808130 does not have an image\n",
      "product 11862486 does not have an image\n",
      "product 11834207 does not have an image\n",
      "product 11859078 does not have an image\n",
      "product 11804071 does not have an image\n",
      "product 11830604 does not have an image\n",
      "product 11540094 does not have an image\n",
      "product 11819712 does not have an image\n",
      "product 11797753 does not have an image\n",
      "product 11877019 does not have an image\n",
      "product 11875818 does not have an image\n",
      "product 11819661 does not have an image\n",
      "product 11804588 does not have an image\n",
      "product 11858019 does not have an image\n",
      "product 11846068 does not have an image\n",
      "product 11846717 does not have an image\n",
      "product 11889460 does not have an image\n",
      "product 11821520 does not have an image\n",
      "product 11825365 does not have an image\n",
      "product 11821208 does not have an image\n",
      "product 11811149 does not have an image\n",
      "product 11856694 does not have an image\n",
      "product 11816276 does not have an image\n",
      "product 11846723 does not have an image\n",
      "product 11865987 does not have an image\n",
      "product 11912840 does not have an image\n",
      "product 11909328 does not have an image\n",
      "product 11909329 does not have an image\n",
      "product 11892381 does not have an image\n",
      "product 11887294 does not have an image\n",
      "product 11922768 does not have an image\n",
      "product 11912637 does not have an image\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save product image features to product-vgg-features.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Preparing the image data ...')\n",
    "\n",
    "# extract VGG16 features\n",
    "def extract_features(dict_product_img):\n",
    "    # model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, nb_channel))\n",
    "    model = applications.VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print(model.summary())\n",
    "    features = dict()\n",
    "    for product_id, img in dict_product_img.items():\n",
    "        feature = model.predict(img, verbose=0)\n",
    "        features[product_id] = feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# Create a dictionary with\n",
    "# key: product id, value: image\n",
    "dict_product_img = dict()\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "img_dir_path = \"data/images/\"\n",
    "dirs = os.listdir(img_dir_path)\n",
    "\n",
    "product_image_feature_file_path = 'product-vgg-features.pkl'\n",
    "\n",
    "for file_name in dirs:\n",
    "    file_path = os.path.join(img_dir_path, file_name)\n",
    "\n",
    "    # img = load_img(file_path)         \n",
    "    img = load_img(file_path, target_size=(img_width, img_height))   # this is a PIL image\n",
    "    x = img_to_array(img)                                            # this is a Numpy array with shape (img_width, img_height, 3)\n",
    "    x = x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))           # this is a Numpy array with shape (1, 3, img_width, img_height)\n",
    "    # x = x.reshape((1,) + x.shape)                                  \n",
    "    # prepare the image for the VGG model\n",
    "    x = preprocess_input(x)\n",
    "    product_id = int(file_name.split('_')[0])\n",
    "\n",
    "    if not int(product_id) in list_product_id:\n",
    "        print('photo {} does not have product information'.format(file_path))\n",
    "    else:\n",
    "        dict_product_img[product_id] = x\n",
    "\n",
    "for product_id in list_product_id_df:\n",
    "    if product_id not in dict_product_img:\n",
    "        print('product {} does not have an image'.format(product_id))\n",
    "\n",
    "# extract VGG16 features\n",
    "dict_product_img_features = extract_features(dict_product_img)\n",
    "# save the features to file\n",
    "dump(dict_product_img_features, open(product_image_feature_file_path, 'wb'))\n",
    "\n",
    "print('save product image features to {}'.format(product_image_feature_file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing text data ...\n",
      "before clean\n",
      "Red cotton, jute and leather large 'Ashby' shoulder bag from Burberry. \n",
      "Blue wool zipped jacket from Closed. \n",
      "Red calf leather 'Rockstud' ballerinas from Valentino Garavani featuring a pointed toe, multiple ankle straps with side fastenings, gold-tone Rockstud embellishments, a brand embossed insole and a flat heel. \n",
      "Sand cotton leopard print shirt from Roberto Cavalli. \n",
      "Black padded jacket  from LOVELESS. \n",
      "cleaned description of product 11295277 is empty\n",
      "after clean\n",
      "red cotton jute and leather large ashby shoulder bag from burberry\n",
      "blue wool zipped jacket from closed\n",
      "red calf leather rockstud ballerinas from valentino garavani featuring pointed toe multiple ankle straps with side fastenings goldtone rockstud embellishments brand embossed insole and flat heel\n",
      "sand cotton leopard print shirt from roberto cavalli\n",
      "black padded jacket from loveless\n",
      "Vocabulary Size: 58\n",
      "save product description to product-descriptions.txt\n"
     ]
    }
   ],
   "source": [
    "print('Preparing text data ...')\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    \"\"\"\n",
    "    https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "    - Convert all words to lowercase.\n",
    "    - Remove all punctuation.\n",
    "    - Remove all words that are one character or less in length (e.g. ‘a’).\n",
    "    - Remove all words with numbers in them.\n",
    "    \"\"\"\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word) > 1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        clean_str = ' '.join(desc)\n",
    "        if not clean_str:\n",
    "            print('cleaned description of product {} is empty'.format(key))\n",
    "        else:\n",
    "            descriptions[key] = clean_str\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    \"\"\"\n",
    "    convert the loaded descriptions into a vocabulary of words\n",
    "    \"\"\"\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "\n",
    "def save_descriptions(descriptions, filename):\n",
    "    \"\"\"\n",
    "    save descriptions to file, one per line\n",
    "    \"\"\"\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        if not desc:\n",
    "            print('product {} does not have a description'.format(key))\n",
    "        # print(key)\n",
    "        # print(desc)\n",
    "        lines.append(str(key) + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "product_description_file_path = 'product-descriptions.txt'\n",
    "\n",
    "# create a dictionary with key: product id -> value: description\n",
    "dict_product_des = dict()\n",
    "\n",
    "for product_id in list_product_id:\n",
    "    # we assume that one product has only one description.\n",
    "    if product_id in dict_product_des:\n",
    "        print('product {} has more than one description'.format(product_id))\n",
    "    description = df_product[df_product['ProductId']==product_id]['Description'].values[0]\n",
    "    if not description:\n",
    "        print('product {} does not have a description'.format(product_id))\n",
    "    else:\n",
    "        dict_product_des[product_id] = description\n",
    "\n",
    "print('before clean')\n",
    "for product_id in list_product_id[0:5]:\n",
    "    print(dict_product_des[product_id])\n",
    "\n",
    "# clean the descriptions\n",
    "clean_descriptions(dict_product_des)\n",
    "\n",
    "print('after clean')\n",
    "for product_id in list_product_id[0:5]:\n",
    "    print(dict_product_des[product_id])\n",
    "\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(dict_product_des)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "# save descriptions\n",
    "save_descriptions(dict_product_des, product_description_file_path)\n",
    "\n",
    "print('save product description to {}'.format(product_description_file_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: prepare the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All image features: 12436\n",
      "Descriptions: 12436\n",
      "Descriptions: train=11192\n",
      "Photos: train=11192\n",
      "Descriptions: test=1244\n",
      "Photos: test=1244\n",
      "Vocabulary Size: 9295\n",
      "Description Length: 98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    load doc into memory\n",
    "    \"\"\"\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_clean_descriptions(filename, list_product_id):\n",
    "    \"\"\"\n",
    "    load clean descriptions into memory\n",
    "    \"\"\"\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        product_id, product_desc = int(tokens[0]), tokens[1:]\n",
    "        if product_id in list_product_id:\n",
    "            if product_id not in descriptions:\n",
    "                descriptions[product_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(product_desc) + ' endseq'\n",
    "            descriptions[product_id].append(desc)\n",
    "\n",
    "    # for key, value in descriptions.items():\n",
    "    #     print(key)\n",
    "    #     print(value)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "\n",
    "def load_photo_features(filename, list_product_id):\n",
    "    \"\"\"\n",
    "    load photo features\n",
    "    \"\"\"\n",
    "    \n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "\n",
    "    # features = {k: all_features[k] for k in list_product_id}\n",
    "\n",
    "    # filter features\n",
    "    dataset = []\n",
    "    # dict_features = dict()\n",
    "    for product_id in list_product_id:\n",
    "        if (str(product_id) in all_features) or (product_id in all_features):\n",
    "            dataset.append(product_id)\n",
    "\n",
    "    # for product_id, features in all_features.items():\n",
    "    #     if int(product_id) in list_product_id:\n",
    "    #         dict_features[int(product_id)] = features\n",
    "\n",
    "    # filter features\n",
    "    features = {int(k): all_features[k] for k in dataset}\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def to_lines(descriptions):\n",
    "    \"\"\"\n",
    "    convert a dictionary of clean descriptions to a list of descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"\n",
    "    fit a tokenizer given caption descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def max_length(descriptions):\n",
    "    \"\"\"\n",
    "    calculate the length of the description with the most words\n",
    "    \"\"\"\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "\n",
    "# ---------\n",
    "# encode the text\n",
    "# for example, the input sequence “little girl running in field” would be split into 6 input-output pairs to train the model:\n",
    "\"\"\"\n",
    "X1,\t\tX2 (text sequence), \t\t\t\t\t\t y (word)\n",
    "photo\tstartseq, \t\t\t\t\t\t\t\t\t little\n",
    "photo\tstartseq, little,\t\t\t\t\t\t\t girl\n",
    "photo\tstartseq, little, girl, \t\t\t\t\t running\n",
    "photo\tstartseq, little, girl, running, \t\t\t in\n",
    "photo\tstartseq, little, girl, running, in, \t\t field\n",
    "photo\tstartseq, little, girl, running, in, field,  endseq\n",
    "\"\"\"\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    \"\"\"\n",
    "    create sequences of images, input sequences and output words for an image\n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    data generator, intended to be used in a call to model.fit_generator()\n",
    "    \"\"\"\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            if key in photos:\n",
    "                photo = photos[key][0]\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "                yield [[in_img, in_seq], out_word]\n",
    "                \n",
    "\n",
    "# image features\n",
    "all_features = load_photo_features(product_image_feature_file_path, list_product_id)\n",
    "print('All image features: %d' % len(all_features))\n",
    "\n",
    "# descriptions\n",
    "all_descriptions = load_clean_descriptions(product_description_file_path, list_product_id)\n",
    "print('Descriptions: %d' % len(all_descriptions))\n",
    "\n",
    "# print('Descriptions')\n",
    "# for key, value in all_descriptions.items():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "\n",
    "# prepare train and test sets\n",
    "percentage_train = 0.9\n",
    "list_train_product_id = list_product_id[0:int(len(list_product_id)*percentage_train)]\n",
    "list_test_product_id = list_product_id[len(list_train_product_id):]\n",
    "\n",
    "train_features = dict()\n",
    "train_descriptions = dict()\n",
    "for product_id in list_train_product_id:\n",
    "    train_features[product_id] = all_features[product_id]\n",
    "    train_descriptions[product_id] = all_descriptions[product_id]\n",
    "\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "test_features = dict()\n",
    "test_descriptions = dict()\n",
    "for product_id in list_test_product_id:\n",
    "    test_features[product_id] = all_features[product_id]\n",
    "    test_descriptions[product_id] = all_descriptions[product_id]\n",
    "\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "\n",
    "# prepare sequences\n",
    "# X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(all_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(all_descriptions)\n",
    "print('Description Length: %d' % max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 98)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 98, 256)      2379520     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 98, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9295)         2388815     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,408,271\n",
      "Trainable params: 6,408,271\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    define the captioning model\n",
    "    \"\"\"\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model ... \n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3204s 286ms/step - loss: 4.4473\n",
      "save model to model_0.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3131s 280ms/step - loss: 3.4604\n",
      "save model to model_1.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3129s 280ms/step - loss: 3.2144\n",
      "save model to model_2.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3127s 279ms/step - loss: 3.0986\n",
      "save model to model_3.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3125s 279ms/step - loss: 3.0369\n",
      "save model to model_4.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3127s 279ms/step - loss: 2.9940\n",
      "save model to model_5.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3124s 279ms/step - loss: 2.9611\n",
      "save model to model_6.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3126s 279ms/step - loss: 2.9435\n",
      "save model to model_7.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3128s 279ms/step - loss: 2.9275\n",
      "save model to model_8.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3127s 279ms/step - loss: 2.9213\n",
      "save model to model_9.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3129s 280ms/step - loss: 2.9141\n",
      "save model to model_10.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3129s 280ms/step - loss: 2.9156\n",
      "save model to model_11.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3130s 280ms/step - loss: 2.9099\n",
      "save model to model_12.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3131s 280ms/step - loss: 2.9119\n",
      "save model to model_13.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3134s 280ms/step - loss: 2.9051\n",
      "save model to model_14.h5\n",
      "Epoch 1/1\n",
      "11192/11192 [==============================] - 3346s 299ms/step - loss: 2.9082\n",
      "save model to model_15.h5\n",
      "Epoch 1/1\n",
      "  333/11192 [..............................] - ETA: 52:50 - loss: 2.9606"
     ]
    }
   ],
   "source": [
    "print('Train model ... ')\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    model_path = 'model_' + str(i) + '.h5'\n",
    "    model.save(model_path)\n",
    "    print('save model to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    # key: product id value: generated description\n",
    "    dict_pred_desc = dict()\n",
    "\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    # for key, desc_list in descriptions.items():\n",
    "    for key, desc in descriptions.items():\n",
    "\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "\n",
    "        # store actual and predicted\n",
    "        # references = [d.split() for d in desc_list]\n",
    "        # print(desc)\n",
    "        references = desc[0].split()\n",
    "        print(references)\n",
    "\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "\n",
    "        print('product id {}'.format(key))\n",
    "        print('description')\n",
    "        print(desc)\n",
    "        print('prediction')\n",
    "        print(yhat)\n",
    "\n",
    "        dict_pred_desc[key] = yhat\n",
    "\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "    return dict_pred_desc\n",
    "\n",
    "\n",
    "print('Evaluate the model ...')\n",
    "filename = 'model_19.h5'\n",
    "model = load_model(filename)\n",
    "\n",
    "# evaluate model\n",
    "dict_pred_desc = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some generated description\n",
    "for product_id in list_test_product_id[0:5]:\n",
    "    imgplot = plt.imshow(dict_product_img[i])\n",
    "    plt.title('[ProductID]{}'.format(product_id))\n",
    "    plt.show()\n",
    "    \n",
    "    print('description')\n",
    "    print(dict_product_des[i])\n",
    "    \n",
    "    print('generated description')\n",
    "    print(dict_pred_desc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
